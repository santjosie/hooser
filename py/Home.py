import streamlit as st
import replicate
from transformers import AutoTokenizer
import os


def sidebar():
    with st.sidebar:
        st.title('Hooser configuration')
        if 'REPLICATE_API_TOKEN' in st.secrets:
            replicate_api_token = st.secrets['REPLICATE_API_TOKEN']
        else:
            replicate_api_token = st.text_input('Enter your Replicate API token:', type='password')
            if not (replicate_api_token.startswith('r8_') and len(replicate_api_token) == 40):
                st.warning('Please enter your Replicate API token.', icon='âš ')

        os.environ['REPLICATE_API_TOKEN'] = replicate_api_token
        st.subheader('Adjust model parameters')
        st.session_state['temperature'] = st.sidebar.slider('Temperature', min_value=0.01, max_value=5.0, value=0.3,
                                                            step=0.1)
        st.session_state['top_p'] = st.sidebar.slider('Top P', min_value=0.01, max_value=1.0, value=0.9, step=0.01)


def main_header():
    st.title('Welcome to Hooser!')
    st.caption('Let Hooser write the user stories for the features that you want to build.')
    st.caption('So you get more time for yoga and travel.')


@st.cache_resource(show_spinner=False)
def get_tokenizer():
    """
    gets the tokenizer used to calculate the number of tokens that the
    prompt uses. caching the info since this does not need to be re-fetched
    during reruns
    """
    return AutoTokenizer.from_pretrained('huggyllama/llama-7b')


def get_num_tokens():
    """
    calculate the length of the prompt in terms of number of tokens.
    to ensure that it stays within the limit of the LLM used.
    """
    lo_tokenizer = get_tokenizer()
    ln_tokens = lo_tokenizer(st.session_state['prompt'])
    return len(ln_tokens)


def generate_arctic_response():
    if get_num_tokens() >= 3072:
        st.error(
            'Length of the prompt is too long. Please shorten it.'
            'Cannot process if the number of tokens generated by the prompt is greater than or equal to 3072')
        st.stop()
    lc_input_prompt = st.session_state['prompt']
    for lc_event in replicate.stream('snowflake/snowflake-arctic-instruct',
                                     input={'prompt': lc_input_prompt,
                                            'prompt_template': r'{lc_input_prompt}',
                                            'temperature': st.session_state['temperature'],
                                            'top_p': st.session_state['top_p']}):
        yield str(lc_event)


def input_prompt():
    """
    draw a container to hold the chat input. if chat input is added to the base page,
    it will be pinned to the bottom of the page. hence wrapping it in a container to bring it to the top.
    """
    with st.container():
        # this is the input field where the user will enter a prompt to generate the user story
        lc_prompt = st.chat_input(placeholder='Write a (very) brief description of the feature '
                                              'you want to build and let Hooser do the rest')
        if lc_prompt:  # checking is user has entered a prompt
            st.session_state['prompt'] = lc_prompt  # assign user entered prompt to the session state attribute - prompt
            lc_response = generate_arctic_response()  # get the response from the LLM
            lc_full_response = st.write_stream(lc_response)
            st.session_state['user_story'] = lc_full_response
        # add response to session state


def response():
    with st.chat_message(name='assistant'):
        st.markdown('User searches for employee record')
        st.markdown('Hello')
        st.markdown('Context')
        st.markdown('Hello')
        st.markdown('Process Flow')
        st.markdown('Login Authentication: User logs into the HCM system using their credentials.')


def main():
    st.set_page_config(
        page_title='Hooser | Home',
        page_icon=':material/home:',
        menu_items={
            'Get help': 'https://www.google.com',
            'About': '# Version: 2.2 #'
        })

    sidebar()
    main_header()
    input_prompt()
    if 'user_story' in st.session_state.keys():
        response()


if __name__ == '__main__':
    main()
